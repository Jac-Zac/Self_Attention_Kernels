#!/bin/bash
#SBATCH --nodes=1                   # 1 node
#SBATCH --ntasks=1                  # total MPI tasks across nodes
#SBATCH --ntasks-per-node=1         # 1 MPI task on the node
#SBATCH --cpus-per-task=24          # OpenMP threads per MPI task
#SBATCH --mem=0                     # use all available memory
##SBATCH --partition=EPYC
#SBATCH --partition=THIN
#SBATCH -t 00:20:00                 # 20 minutes
##SBATCH --exclusive                 # Get exclusive access to benchmark
#SBATCH --job-name=self_attention

# Set the executable name
EXEC=./cmhsa.out

# =======================================================
# Set OpenMP environment variables
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
export OMP_PLACES=cores
export OMP_PROC_BIND=close
export OMP_DISPLAY_AFFINITY=TRUE

# Loading necessary module
# module load openMPI/5.0.5

# Set the total number of MPI tasks
TOTAL_MPI_TASKS=${SLURM_NTASKS}

# Define the arguments for the executable
# A medium-sized grid suitable for a single-node run
ARGS="--batch 2 --n_heads 4 --seq_len 4096 --head_dim 64"

# Print run information
echo "Running on a single node with ${TOTAL_MPI_TASKS} MPI tasks."
echo "Each task uses ${OMP_NUM_THREADS} OpenMP threads."

# Minimal Python venv + requirements
VENV_DIR="${VENV_DIR:-.venv}"
PY_BIN="${PY_BIN:-$(command -v python3 || command -v python)}"
if [ -z "$PY_BIN" ]; then
  echo "Python not found. Load a Python module." >&2
  exit 1
fi
[ -d "$VENV_DIR" ] || "$PY_BIN" -m venv "$VENV_DIR"
# shellcheck disable=SC1090
source "$VENV_DIR/bin/activate"
python -m pip install --upgrade pip

# Pick requirements.txt and install always
REQ_FILE="${REQ_FILE:-$(git rev-parse --show-toplevel 2>/dev/null || pwd)/requirements.txt}"
[ -f "$REQ_FILE" ] || REQ_FILE="requirements.txt"
if [ -f "$REQ_FILE" ]; then
  echo "Installing Python dependencies from $REQ_FILE..."
  pip install -r "$REQ_FILE"
else
  echo "requirements.txt not found; skipping pip install."
fi

source .venv/bin/activate

# Compile and run the benchmark
# USE_SRUN=1 ensures child processes spawned by Python get proper SLURM CPU binding

export OMP_NUM_THREADS=1
make benchmark BENCH_BACKEND=multi BENCH_THREADS=${OMP_NUM_THREADS} USE_SRUN=1
export OMP_NUM_THREADS=2
make benchmark BENCH_BACKEND=multi BENCH_THREADS=${OMP_NUM_THREADS} USE_SRUN=1
export OMP_NUM_THREADS=8
make benchmark BENCH_BACKEND=multi BENCH_THREADS=${OMP_NUM_THREADS} USE_SRUN=1
export OMP_NUM_THREADS=12
make benchmark BENCH_BACKEND=multi BENCH_THREADS=${OMP_NUM_THREADS} USE_SRUN=1
export OMP_NUM_THREADS=18
make benchmark BENCH_BACKEND=multi BENCH_THREADS=${OMP_NUM_THREADS} USE_SRUN=1
export OMP_NUM_THREADS=24
make benchmark BENCH_BACKEND=multi BENCH_THREADS=${OMP_NUM_THREADS} USE_SRUN=1

# make multi DEBUG=1 VERSION=v1 BENCH_THREADS=${OMP_NUM_THREADS}
# Run the code using mpirun
# mpirun -np ${TOTAL_MPI_TASKS} --bind-to none  ${EXEC} ${ARGS} > ${TOTAL_MPI_TASKS}_mpi_${OMP_NUM_THREADS}_threads.log

# perf stat -d -d -e instructions,cycles,cache-references,cache-misses,dTLB-loads,dTLB-load-misses,iTLB-loads,iTLB-load-misses -a $EXEC $ARGS
