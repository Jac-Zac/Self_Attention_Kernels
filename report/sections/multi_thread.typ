= Multi-Threaded CPU Implementation  

The multi-threaded benchmarks were run on Orfeo's EPYC partition.

Benchmark configuration used in this chapter:
`BENCH_BATCH=4`, `BENCH_HEADS=32`, `BENCH_SEQLEN=4096`, `BENCH_ITERS=10`.

Code: #link("https://github.com/Jac-Zac/Self_Attention_Kernels/tree/main/kernels/multi_thread")[github.com/Jac-Zac/Self_Attention_Kernels/tree/main/kernels/multi_thread].

== Basic Parallelization (v0)

Version #link("https://github.com/Jac-Zac/Self_Attention_Kernels/blob/main/kernels/multi_thread/v0.cpp")[v0] implements straightforward multi-threading by parallelizing over the batch and heads dimensions.
The parallelization strategy uses `collapse(2)` to create 4 × 32 = 128 independent work units (one for each batch-head pair), providing good load balance across threads.
Each thread allocates its own scratch space for attention weights using `thread_id * seq_len_padded`, avoiding contention between threads.
The implementation follows the same algorithm as the optimized single-threaded v1, including fused max-finding with score computation, causal masking, and multiplication by inverse for normalization.
The benchmarks use OpenMP environment variables `OMP_PLACES=cores` and `OMP_PROC_BIND=close`.

Despite inheriting all single-threaded optimizations (aligned memory access, padded strides, and full `-ffast-math` vectorization), v0 still somewhat lags behind the most efficient implementation.
With the larger problem size (4096 sequence length), memory traffic becomes a big bottleneck, motivating the tiling approach in v1.

== Q-Tiling for Cache Reuse (v1)

Version #link("https://github.com/Jac-Zac/Self_Attention_Kernels/blob/main/kernels/multi_thread/v1.cpp")[v1] introduces query tiling to improve cache locality and reduce memory traffic.
The key idea is to process `TILE_Q = 32` queries together, allowing each key row to be loaded once and reused for all queries in the tile that need it.

The algorithm consists of three passes:

1. *Pass 1:* Compute $Q K^T$ scores for the tile.
   By iterating over keys in the outer loop, each K row is loaded once per tile instead of once per query, reducing K memory traffic.
   Causal masking is handled by `q_start = max(key_pos, q_tile)`, which skips masked positions.

2. *Pass 2:* Numerically stable softmax.
   Each query computes its own max and sum of exponentials independently.

3. *Pass 3:* Weighted V accumulation.
   Same loop pattern as Pass 1, with each V row loaded once per tile.

The implementation adds `ASSUME_ALIGNED` hints on rows to improve vectorization, similar to the single-threaded versions.

=== Strong Scaling Results

The strong scaling benchmark was run on Orfeo's EPYC partition (AMD EPYC 9654, 128 cores) with the following configuration: `batch=4`, `n_heads=32`, `seq_len=4096`, `iters=10`.
Thread counts were varied from 1 to 128 to evaluate parallel efficiency.

#figure(
  image("../figures/benchmark_strong_scaling.png", width: 95%),
  caption: [Strong scaling results for multi-threaded kernels. *Left:* speedup relative to single-threaded baseline (ideal scaling shown as dashed line). *Right:* execution time.],
) <fig:benchmark_strong_scaling>

 Detailed results are available in @tab:benchmark_strong_scaling in the appendix. The v1 kernel achieves near-ideal scaling up to 64 threads, with a speedup of approximately 48× over the single-threaded baseline (20.613019 s → 0.428888 s). These are CPU multi-threaded results measured on Orfeo's EPYC partition and are separate from the GPU timings reported in the CUDA section.
 At 128 threads, v1 remains slightly slower than the PyTorch GPU SDPA baseline (GPU and CPU timings are not directly comparable). Compared to the single-threaded v0 baseline, v1 shows substantial improvement due to cache-aware tiling and reduced memory traffic.
